## 1 线性回归简介

利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方法。

一元线性回归：y = wx + b

多元线性回归： $h_{(w)} = w_1 x_1 + w_2 x_2 + w_3 x_3 + \cdots + b = \mathbf{w}^T \mathbf{x} + b$

其中， $\mathbf{w} = \begin{pmatrix} b \\ w_1 \\ w_2 \\ \vdots \end{pmatrix}, \quad \mathbf{x} = \begin{pmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \end{pmatrix}$

根据矩阵运算：$\mathbf{w}^T \mathbf{x} = [b, w_1, w_2, \cdots] \cdot \begin{pmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \end{pmatrix}$

> 我感觉这里表达有问题，w 和 x 里不应该有 b 和 1，否则和外面加的 b 重复了。

## 2 线性回归问题的求解

### 2.1 线性回归问题流程

![[Pasted image 20250425234711.png]]

### 2.2 损失函数

衡量每个样本预测值与真实值差异的函数，也叫代价函数、成本函数、目标函数。

下面的正规方程法和梯度下降法就是为了降低损失的。

损失怎么算？

**最小二乘（Least Squares Method）**：

$J (w, b) = \sum_{i=0}^{m} \left (h\left (x^{(i)}\right) - y^{(i)}\right)^2$

即每个样本的（真实值 - 预测值）来形成损失函数。会得到一个只有参数的方程，此时，如何让损失函数值取到最小时，得到的参数就是最优解。

![[Pasted image 20250425233523.png]]

但是，最小二乘法有一个问题，随着样本数量的增加或减少，损失值也会相应的增加或减少，比如 10 个样本算出来的损失值，与 100 个样本算出来的损失值，一定是不一样的。也就意味着样本越多损失越大，这不是我们想要的。

因此可以求一下平均，得到：

**均方误差（Mean-Square Error, MSE）**：

$J (w, b) = \frac{1}{m} \sum_{i=0}^{m} \left (h\left (x^{(i)}\right) - y^{(i)}\right)^2 = \sum_{i=0}^{m} (kx^{(i)} + b - y^{(i)})^2$

**平均绝对误差（Mean Absolute Error, MAE）**：

$J (w, b) = \frac{1}{m} \sum_{i=0}^{m} \left| h\left (x^{(i)}\right) - y^{(i)} \right|$

![[Pasted image 20250426235313.png]]

![[Pasted image 20250426235549.png]]

![[Pasted image 20250426235613.png]]

![[Pasted image 20250427000041.png]]

### 2.3 正规方程法（略）

其实就是正着求损失函数的各参数（k、b）的偏导，然后让偏导等于 0，求出 k 和 b。

### 2.4 梯度下降法

梯度下降法是一种寻找使损失函数最小化的方法。从数学上的角度来看，梯度的方向是函数增长速度最快的方向，那么梯度的反方向就是函数减少最快的方向

#### 2.4.1 什么是梯度 (gradient/grad)

- **单变量函数**中，梯度就是某一点切线斜率（某一点的导数）；方向为函数增长最快的方向。
- **多变量函数**中，梯度就是某一个点的偏导数；方向为偏导数分量的向量方向。

#### 2.4.2 梯度下降公式

循环迭代求当前点的梯度，更新当前的权重参数：

  $\theta_{i+1} = \theta_i - \alpha \frac{\partial}{\partial \theta_i} J(\theta)$

- $\alpha$：学习率（步长），不能太大也不能太小（机器学习中常用范围：0.001 ~ 0.01）。
	- 学习率太小：
		- 收敛速度过慢
		- 需要更多训练时间（"very small learning rate needs lots of iterations"）
	- 学习率太大（"too big learning rate"）：
		- 容易错过最优解
		- 可能引起震荡
		- 甚至导致梯度爆炸
	- ![[Pasted image 20250504191815.png]]
- 梯度是上升最快的方向，而我们需要下降最快的方向，因此公式中需**加负号**。

![[Pasted image 20250504190711.png]]

#### 2.4.3 梯度下降优化步骤

1. 给定初始位置、步长（学习率 $\alpha$）
2. 计算当前点的梯度负方向 $- \nabla J(\theta)$
3. 向负梯度方向移动步长 $\theta_{i+1} = \theta_i - \alpha \nabla J(\theta)$
4. 重复 2-3 步直至收敛，判断依据可以是以下任意一种：
   - 两次迭代差距小于指定阈值
   - 达到最大迭代次数（一般选这个）

![[Pasted image 20250427000200.png]]

![[Pasted image 20250427000206.png]]

![[Pasted image 20250427000212.png]]

![[Pasted image 20250504185448.png]]

在进行模型训练时，有三个基础的概念：

1. **Epoch**: 使用全部数据对模型进行以此完整训练，训练轮次
2. **Batch_size**: 使用训练集中的小部分样本对模型权重进行以此反向传播的参数更新，每次训练每批次样本数量
3. **Iteration**: 使用一个 Batch 数据对模型进行一次参数更新的过程

假设数据集有 50000 个训练样本，现在选择 Batch Size = 256 对模型进行训练。

- 每个 Epoch 要训练的图片数量：50000
- 训练集具有的 Batch 个数：50000/256+1=196
- 每个 Epoch 具有的 Iteration 个数：196
- 10 个 Epoch 具有的 Iteration 个数：1960

## 3 线性回归 API 和案例

### 3.1 线性回归 API

### 3.2 案例波士顿房价预测

## 4 欠拟合与过拟合

### 4.1 出现原因

### 4.2 解决方法

![[Pasted image 20250427001836.png]]

![[Pasted image 20250427001851.png]]

### 4.3 L1 正则化

![[Pasted image 20250427001945.png]]

![[Pasted image 20250427002104.png]]

![[Pasted image 20250427002831.png]]

### 4.4 L2 正则化

![[Pasted image 20250427002813.png]]

![[Pasted image 20250427002854.png]]
