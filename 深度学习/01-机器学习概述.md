## 1 机器学习常用术语 - 数据集描述

| 姓名  | 年龄  | 薪资    |
| --- | --- | ----- |
| 张三  | 18  | 20000 |
| 李四  | 20  | 50000 |
| 王五  | 19  | ？     |

- 样本（sample）：一行数据就是一个样本（记录）；
- 特征（feature）：一列数据就是一个特征（属性）；
- 标签/目标（label/target）：模型要预测的那一列数据。这里是薪资。
- 数据集：多个样本组成数据集
	- 训练集（training set）：有时再划分出一部分验证集
	- 测试集（testing set）
	- 比例：8:2、7:3

## 2 机器学习算法分类

### 2.1 有监督学习

输入：特征值、标签。

> 动物分类。

#### 2.1.1 分类任务（Classification）

标签值（目标值）是不连续的。

- 二分类
- 多分类

#### 2.1.2 回归任务 （Regression）

标签值（目标值）是连续的。

> 房价预测。

### 2.2 无监督学习

输入：特征值（没有标签），根据样本间的相似性，对样本集聚类（Clustering），以发现事物内部结构及相互关系。

> 物以类聚，人以群分。

### 2.3 半监督学习

输入：特征值、一部分有标签。

1. 让专家标注少量数据，利用已经标记的数据（也就是带有类标签）训练出一个模型。
2. 再利用该模型去套用未标记的数据。（但不一定可靠）
3. 通过询问领域专家分类结果，与模型分类结果做对比，从而对模型做改善。（人工修正）

半监督学习方式可大幅降低标记成本。

> 医生（专家）标记少量病理图，然后后面训练出模型后，还需要医生再根据模型输出做人工修正。

### 2.4 强化学习

环境反馈给算法状态，算法根据状态做出相应动作，如果正确则给奖励（减小损失值）。

![[Pasted image 20250424200325.png]]

> 应用场景：AlphaGo 围棋、各类游戏、对抗比赛、无人驾驶场景、ChatGPT。

## 3 机器学习建模流程

- 获取数据
	- 获取经验数据
	- 图像数据
	- 文本数据
- **数据基本处理**
	- 数据缺失值处理
	- 异常值处理
- **特征工程**
	- 特征提取
	- 特征预处理
	- 特征降维
- **机器学习**（模型训练）
	- 线性回归
	- 逻辑回归
	- 决策树
	- GBDT
- **模型评估**
	- 回归评测指标
	- 分类评测指标
	- 聚类评测指标
- 在线服务

## 4 特征工程

利用专业背景知识和技巧**处理数据**，让机器学习算法效果最好。这个过程就是特征工程（Feature Engineering）。

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

- 特征提取（feature extraction）：从原始的冗余的数据中，提取出我需要的特征的特征值，完成样本中特征值的部分（特征向量）。
- **特征预处理**（feature preprocessing）：特征对模型产生影响，因量纲（单位）问题，有些特征对模型影响大（20000 元）、有些影响小（18 岁）。可以通过归一化等预处理来解决。
- **特征降维**（feature decomposition）：将原始数据的维度降低，比如 3D -> 2D。会改变原始数据。
- 特征选择（featrue selection）：原始数据特征很多，与任务相关是其中一个特征集合子集，不会改变原始数据。
- 特征组合（feature crosses）：把多个特征合并成一个特征，通常利用乘法或加法完成。

### 4.1 特征预处理

特征的单位或者大小相差较大，或某特征的方差相比其他的特征要大出几个数量级，容易影响目标结果，是的一些模型（算法）无法学习到其他的特征。

#### 4.1.1 归一化

把原始数据映射到 `[mi, mx]` 之间（默认为 `[0, 1]`）。

- 先映射到 `[0, 1]` 之间：$X' = \frac{x - \min}{\max - \min}$
- 再映射到 `[mi, mx]` 之间：$X'' = X' \times (\text{mx} - \text{mi}) + \text{mi}$

但是，归一化受最大最小值影响，如果特征里有一个异常值，则会受影响，鲁棒性较差。而且也不适合大规模数据的使用。只适合传统精确小数据场景。一般，图像可以用归一化，因为再 0 到 255 之间。

#### 4.1.2 标准化

将原始数据转换为均值为 0、标准差为 1 的标准正态分布的数据。

$X' = \frac{x - \mu}{\sigma}$，其中 $\mu$（mean）为特征的平均值，$\sigma$ 为特征的标准差。

即使有异常值，有特别大或特别小的值，也会在 $3\sigma$ 之外，少量的异常点对平均值影响不大，适合现代嘈杂大数据场景，以后一般就用这个。

#### 4.1.3 API

```python
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler

def 归一化():
    # 准备数据
    data = [
        [90, 2, 10, 40],
        [60, 4, 15, 45],
        [75, 3, 13, 46]
    ]
    # 初始化归一化对象
    transformer = MinMaxScaler()
    # 对原始特征进行变换
    data = transformer.fit_transform(data)
    # 打印归一化后的结果
    print(data)
    # [[1.         0.         0.         0.]
    #  [0.         1.         1.         0.83333333]
    #  [0.5        0.5        0.6        1.]]

def 标准化():
    # 准备数据
    data = [
        [90, 2, 10, 40],
        [60, 4, 15, 45],
        [75, 3, 13, 46]
    ]
    # 初始化标准化对象
    transformer = StandardScaler()
    # 对原始特征进行变换
    data = transformer.fit_transform(data)
    # 打印标准化后的结果
    print(data)
    print(transformer.mean_) # 均值
    print(transformer.var_) # 标准差
    # [[ 1.22474487  -1.22474487  -1.29777137  -1.3970014]
    #  [-1.22474487   1.22474487   1.13554995   0.50800051]
    #  [ 0.           0.           0.16222142   0.88900089]]
    # [ 75.           3.          12.66666667  43.66666667]
    # [150.           0.66666667   4.22222222   6.88888889]

if __name__ == '__main__':
    归一化()
    标准化()
```

## 5 模型拟合

- 拟合（fitting）：表示模型对样本点的拟合情况。
- 欠拟合（under-fitting）：模型在训练集上表现很差，在测试集上表现很差。
	- 原因：模型太过于简单。
- 过拟合（over-fitting）：模型在训练集上表现很好，在测试集上表现很差。
	- 原因：模型太过于复杂、数据不纯、训练数据太多。
- 泛化（generalization）：模型在新数据集（非训练数据）上的表现好坏的能力。
- 奥卡姆剃刀原则：给定两个具有相同泛化误差的模型，较简单的模型比复杂模型更可取。

## 6 基于 Python 的 scikit-learn 库

```bash
pip install scikit-learn
```

- Classification（有监督分类）
- Regression（有监督回归）
- Clustering（无监督聚簇）
- Preprocessing（特征预处理）
- Dimensionality reduction（特征降维）
- Model selection（模型评估）

![[Pasted image 20250424210723.png]]

## 7 距离度量

### 7.1 欧式距离

欧氏距离（Euclidean Distance）：

- 二维平面上两点的欧式距离：$d_{12} = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$
- 三维平面上两点的欧式距离：$d_{12} = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2 + (z_1 - z_2)^2}$
- N 维平面上两点的欧式距离：$d_{12} = \sqrt{\sum_{k=1}^{n} (x_{1k} - x_{2k})^2}$

![[Pasted image 20250424224346.png]]

### 7.2 曼哈顿距离

曼哈顿距离（Manhattan Distance）：也称为“城市街区距离”（City Block Distance），曼哈顿城市特点横平竖直。**只能沿着 x 或 y 走**。

- 二维平面上两点的曼哈顿距离： $d_{12} = \left| x_1 - x_2 \right| + \left| y_1 - y_2 \right|$
- 三维平面上两点的曼哈顿距离：$d_{12} = |x_1 - x_2| + |y_1 - y_2| + |z_1 - z_2|$
- N 维平面上两点的曼哈顿距离：$d_{12} = \sum_{k=1}^{n} |x_{1k} - x_{2k}|$

![[Pasted image 20250424224735.png]]

### 7.3 切比雪夫距离

切比雪夫距离（Chebyshev Distance）：国际象棋中，国王可以直行、横行、斜行，走一步可以移动到相邻 8 个方格中的任意一个。

- 二维平面上两点的切比雪夫距离：$d_{12} = \max\left(|x_1 - x_2|, |y_1 - y_2|\right)$
- 三维平面上两点的切比雪夫距离：$d_{12} = \max\left(|x_1 - x_2|, |y_1 - y_2|, |z_1 - z_2|\right)$
- N 维平面上两点的切比雪夫距离：$d_{12} = \max_{1 \leq i \leq n} |x_{1i} - x_{2i}|$

![[Pasted image 20250424230606.png]]

### 7.4 闵可夫斯基距离

闵可夫斯基距离（Minkowski Distance）：又称“闵氏距离”，不是一种新的距离度量方式，是对多个公式（欧式、曼哈顿）的概括性表述。

$d_{12} = \sqrt[p]{\sum_{k=1}^{n} |x_{1k} - x_{2k}|^p}$

- 当 p = 1 时，为曼哈顿距离；
- 当 p = 2 时，为欧式距离；
- 当 p -> ∞ 时，为切比雪夫距离。

## 8 超参数选择方法

### 8.1 交叉验证

交叉验证（Cross Validation）是一种数据集的分割方法，将训练集划分为 n 份，拿出一份做验证集，其他 n-1 份做训练集。称为 n 折交叉验证。

如下，将数据集划分为 cv = 4 份：

![[Pasted image 20250425165810.png]]

总共训练 4 次，评估 4 次。取平均值作为模型得分。

### 8.2 网格搜索

只需要将若干参数传递给网格搜索对象，它自动帮我们完成不同超参数的组合、模型训练、模型评估，最终返回一组最优的超参数。

比如，一个算法里的超参数有 k 和 m，则我们可以设定：k 属于 {3, 4, 5}，m 属于 {3, 9}，我们配合四折交叉验证，那么会有：

```text
k = 3, m = 3, 四折交叉验证，80%
k = 4, m = 3, 四折交叉验证，80%
k = 5, m = 3, 四折交叉验证，80%
k = 3, m = 9, 四折交叉验证，80%
k = 4, m = 9, 四折交叉验证，80%
k = 5, m = 9, 四折交叉验证，80%
```

**网格搜索 + 交叉验证**的强力组合（模型选择和调优）：

- 交叉验证解决模型的数据输入问题（数据集划分）得到更可靠的模型。
- 网格搜索解决超参数的组合。
- 两个组合在一起形成一个模型参数调优的解决方案。

### 8.3 交叉验证网格搜索 API

```python
# 4.模型训练
# 4.1.初始化模型
estimator = KNeighborsClassifier()   
# 4.2.网格搜索交叉验证
param_grid = {'n_neighbors': [1, 3, 5, 7]}
estimator = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5) # 给estimator附魔，网格搜索交叉验证
# 4.3.训练模型
estimator.fit(X_train, y_train) # 4个模型，每个模型5折交叉验证
print('最佳参数：', estimator.best_params_)
print('最佳结果：', estimator.best_score_)
print('最佳估计器：', estimator.best_estimator_)
print('交叉验证结果：', estimator.cv_results_)
# 4.4.保存交叉验证结果
cv_results = pd.DataFrame(estimator.cv_results_)
cv_results.to_csv(path_or_buf='./gridsearchcv_results.csv')
```
