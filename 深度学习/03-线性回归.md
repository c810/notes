## 1 线性回归简介

利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方法。

一元线性回归：y = wx + b

多元线性回归： $h_{(w)} = w_1 x_1 + w_2 x_2 + w_3 x_3 + \cdots + b = \mathbf{w}^T \mathbf{x} + b$

其中， $\mathbf{w} = \begin{pmatrix} b \\ w_1 \\ w_2 \\ \vdots \end{pmatrix}, \quad \mathbf{x} = \begin{pmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \end{pmatrix}$

根据矩阵运算：$\mathbf{w}^T \mathbf{x} = [b, w_1, w_2, \cdots] \cdot \begin{pmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \end{pmatrix}$

> 我感觉这里表达有问题，w 和 x 里不应该有 b 和 1，否则和外面加的 b 重复了。

## 2 线性回归问题的求解

### 2.1 线性回归问题流程

![[Pasted image 20250425234711.png]]

### 2.2 损失函数

衡量每个样本预测值与真实值差异的函数，也叫代价函数、成本函数、目标函数。

下面的正规方程法和梯度下降法就是为了降低损失的。

损失怎么算？

**最小二乘（Least Squares Method）**：

$J (w, b) = \sum_{i=0}^{m} \left (h\left (x^{(i)}\right) - y^{(i)}\right)^2$

即每个样本的（真实值 - 预测值）来形成损失函数。会得到一个只有参数的方程，此时，如何让损失函数值取到最小时，得到的参数就是最优解。

![[Pasted image 20250425233523.png]]

但是，最小二乘法有一个问题，随着样本数量的增加或减少，损失值也会相应的增加或减少，比如 10 个样本算出来的损失值，与 100 个样本算出来的损失值，一定是不一样的。也就意味着样本越多损失越大，这不是我们想要的。

因此可以求一下平均，得到：

**均方误差（Mean-Square Error, MSE）**：

$J (w, b) = \frac{1}{m} \sum_{i=0}^{m} \left (h\left (x^{(i)}\right) - y^{(i)}\right)^2 = \sum_{i=0}^{m} (kx^{(i)} + b - y^{(i)})^2$

**平均绝对误差（Mean Absolute Error, MAE）**：

$J (w, b) = \frac{1}{m} \sum_{i=0}^{m} \left| h\left (x^{(i)}\right) - y^{(i)} \right|$

### 2.3 正规方程法（略）

其实就是正着求损失函数的各参数（k、b）的偏导，然后让偏导等于 0，求出 k 和 b。

### 2.4 梯度下降法

## 3 回归模型评估方法

### 3.1 MAE

### 3.2 MSE

### 3.3 RSM

## 4 线性回归 API 和案例

### 4.1 线性回归 API

### 4.2 案例波士顿房价预测

## 5 欠拟合与过拟合

### 5.1 出现原因

### 5.2 解决方法

### 5.3 L1 正则化

### 5.4 L2 正则化
